{"cells":[{"cell_type":"markdown","metadata":{"id":"c0xc1jf2OfX6"},"source":["#Setting up environment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3528,"status":"ok","timestamp":1655043318809,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"},"user_tz":-480},"id":"zUBYYldeTcKj","outputId":"2fb47dcc-d88b-4db5-ce52-ec3e8034ae62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opencc\n","  Downloading OpenCC-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (769 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 769 kB 8.2 MB/s \n","\u001b[?25hInstalling collected packages: opencc\n","Successfully installed opencc-1.1.4\n"]}],"source":["!pip install opencc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2493,"status":"ok","timestamp":1655017811404,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"},"user_tz":-480},"id":"zv-WJlTvOSja","outputId":"0b082418-df19-4ffa-8489-049e919ccf76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/original')\n","path = os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z04zmMnOeQq"},"outputs":[],"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"B43oNE22O69X","executionInfo":{"status":"ok","timestamp":1655043319257,"user_tz":-480,"elapsed":485,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import jieba\n","from opencc import OpenCC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mfjBY61LqtT"},"outputs":[],"source":["!pip install -U spacy\n","!python -m spacy download zh_core_web_sm\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","source":["from gensim.models import word2vec"],"metadata":{"id":"5RTOV_xcYqso","executionInfo":{"status":"ok","timestamp":1655043368013,"user_tz":-480,"elapsed":457,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["tf."],"metadata":{"id":"Q6QJzFsARoUZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"07TcsQmlC1MQ"},"source":["# WORD2VEC TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7PVcgTJC5O1"},"outputs":[],"source":["os.chdir(os.path.join(path, 'dataset'))\n","!wget \"https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2\"\n","os.chdir(os.path.join(path, '..'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OXgHd8AfKIAS"},"outputs":[],"source":["import spacy\n","\n","nlp_zh = spacy.load(\"zh_core_web_sm\")\n","nlp_en = spacy.load(\"en_core_web_sm\")\n","\n","STOPWORDS =  nlp_zh.Defaults.stop_words | \\\n","        nlp_en.Defaults.stop_words | \\\n","        set([\"\\n\", \"\\r\\n\", \"\\t\", \" \", \"\"])\n","\n","cc = OpenCC('s2t')\n","for word in STOPWORDS.copy():\n","  STOPWORDS.add(cc.convert(word))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8gSai4gK3nO"},"outputs":[],"source":["def preprocess_and_tokenize( text, token_min_len = 1, token_max_len = 15, lower = True):\n","    if lower: \n","      text = text.lower()\n","    text = cc.convert(text)\n","    return [\n","        token for token in jieba.cut(text, cut_all = False)\n","        if token_min_len <= len(token) <= token_max_len and \\\n","            token not in STOPWORDS\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9hxDuorLJcr"},"outputs":[],"source":["import gensim\n","from gensim.corpora import WikiCorpus\n","\n","wiki_corpus = WikiCorpus(os.path.join(path, 'dataset', 'zhwiki-latest-pages-meta-history1.xml-p10212p26672.bz2'), tokenizer_func = preprocess_and_tokenize, token_min_len = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYZmwhxnfZue"},"outputs":[],"source":["from datetime import datetime as dt\n","generator = wiki_corpus.get_texts()\n","\n","with open(os.path.join(path, 'dataset', 'wiki_corpused.txt'), \"w\", encoding = 'utf-8') as output:\n","  for texts_num, tokens in enumerate(generator):\n","    output.write(\" \".join(tokens) + \"\\n\")\n","    if (texts_num + 1) % 10000 == 0:\n","      print(f\"[{str(dt.now()):.19}] å·²å¯«å…¥ {texts_num} ç¯‡æ–·è©žæ–‡ç« \")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6628875,"status":"ok","timestamp":1655009360704,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"},"user_tz":-480},"id":"6awepUsKmweE","outputId":"fd3cc105-a46c-4160-ab50-820074439ef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Use 2 workers to train Word2Vec (dim=300)\n"]}],"source":["import multiprocessing\n","max_cpu_counts = multiprocessing.cpu_count()\n","word_dim_size = 300\n","print(f\"Use {max_cpu_counts} workers to train Word2Vec (dim={word_dim_size})\")\n","\n","sentences = word2vec.LineSentence(os.path.join(path, 'dataset', 'wiki_corpused.txt'))\n","\n","model = word2vec.Word2Vec(sentences, size = word_dim_size, workers = max_cpu_counts)\n","\n","output_model = f\"word2vec.zh.{word_dim_size}.model\"\n","model.save(os.path.join(path, f\"word2vec.zh.{word_dim_size}.model\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24749,"status":"ok","timestamp":1655016046961,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"},"user_tz":-480},"id":"ZyZ7mZysnYoP","outputId":"a2e04547-8534-425b-cc21-7c915833493d"},"outputs":[{"output_type":"stream","name":"stdout","text":["ç¸½å…±æ”¶éŒ„äº† 1138562 å€‹è©žå½™\n"]}],"source":["w2v_model = word2vec.Word2Vec.load(os.path.join(path, \"word2vec.zh.300.model\"))\n","print(f\"ç¸½å…±æ”¶éŒ„äº† {len(w2v_model.wv.vocab)} å€‹è©žå½™\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMa8-Jm2n9KT"},"outputs":[],"source":["w2v_model.wv.most_similar(\"kerkhove\", topn=10)"]},{"cell_type":"markdown","metadata":{"id":"CgC4m-2SOqsl"},"source":["# Combine data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ez7_GJpNOzE-"},"outputs":[],"source":["file_path = os.path.join(path, 'dataset')\n","df = pd.DataFrame(columns = ['content', 'label'])\n","cc = OpenCC('s2t')\n","for root, _, files in os.walk(os.path.join(file_path, 'ch_fake')):\n","  for file_name in files:\n","    with open(os.path.join(root, file_name)) as f:\n","      tmp = {'content' : cc.convert(f.read()), 'label' : 0}\n","      df = df.append(tmp, ignore_index = True)\n","\n","for root, _, files in os.walk(os.path.join(file_path, 'ch_real')):\n","  for file_name in files:\n","    with open(os.path.join(root, file_name)) as f:\n","      tmp = {'content' : cc.convert(f.read()), 'label' : 1}\n","      df = df.append(tmp, ignore_index = True)\n","      \n","df.to_csv(os.path.join(file_path, 'data.csv'), encoding = 'utf-8_sig')"]},{"cell_type":"markdown","source":["# DATA PREPROCESSING\n"],"metadata":{"id":"Kz6ZYOcjWEWL"}},{"cell_type":"code","source":["import spacy\n","from opencc import OpenCC\n","nlp_zh = spacy.load(\"zh_core_web_sm\")\n","nlp_en = spacy.load(\"en_core_web_sm\")\n","\n","STOPWORDS =  nlp_zh.Defaults.stop_words | \\\n","        nlp_en.Defaults.stop_words | \\\n","        set([\"\\n\", \"\\r\\n\", \"\\t\", \" \", \"\"])\n","\n","cc = OpenCC('s2t')\n","for word in STOPWORDS.copy():\n","  STOPWORDS.add(cc.convert(word))"],"metadata":{"id":"4Yb75A04WggM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pn4VoaZELhUk"},"outputs":[],"source":["def preprocess(df, stop_words):\n","  import re\n","  for i, data in enumerate(df['content']):\n","    data = data.lower()\n","    data = re.sub('[a-zA-Z0-9ã€€ðŸ†™\\-.]', '', data)\n","    seg = jieba.cut(data, cut_all = False)\n","    seg = [v for v in seg if v not in STOPWORDS]\n","    df.loc[i, 'content'] = ' '.join(seg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLSR8I7mXcGB"},"outputs":[],"source":["df = pd.read_csv(os.path.join(path, 'dataset', 'data.csv'))\n","preprocess(df, STOPWORDS)"]},{"cell_type":"code","source":["from sklearn.utils import shuffle\n","df = shuffle(df)\n","\n","train_df = df[ :150]\n","test_df = df[150: ]\n","train_x, test_x = train_df['content'].str.split(), test_df['content'].str.split()\n","train_y, test_y = train_df['label'], test_df['label']"],"metadata":{"id":"nYqN2KJjp-8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings_index = dict()\n","with open('keras_word2vec.txt') as f:\n","  for line in f:\n","    values = line.split()\n","    if len(values) != 300 + 1:\n","      continue\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","  f.close()"],"metadata":{"id":"kfX0Xkritfke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(df['content'])\n","sequences = tokenizer_obj.texts_to_sequences(df['content'])\n","review_pad = pad_sequences(sequences, maxlen = 256)\n","\n","word_index = tokenizer_obj.word_index\n","\n","vocab_size = len(word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))\n","\n","for word, i in tokenizer_obj.word_index.items():\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"],"metadata":{"id":"mkOlbbLRvL59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkwahkxbyuWf","executionInfo":{"status":"ok","timestamp":1655018658357,"user_tz":-480,"elapsed":79,"user":{"displayName":"çŽ‹ä¿Šé–”","userId":"00929504554322960712"}},"outputId":"c6d2611c-0601-47df-e5fd-218c48ff54b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12159"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["train_sequence = tokenizer_obj.texts_to_sequences(train_x)\n","train_padded = pad_sequences(train_sequence, maxlen = 256)\n","\n","test_sequence = tokenizer_obj.texts_to_sequences(test_x)\n","test_padded = pad_sequences(test_sequence, maxlen = 256)\n"],"metadata":{"id":"wMb6nf1ZyBEP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GRU\n"],"metadata":{"id":"kUSil01_nM_c"}},{"cell_type":"code","source":["w2v_model.wv.save_word2vec_format('keras_word2vec.txt', binary=False)"],"metadata":{"id":"9j6ULQAbtW7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KttCjD2lLoo1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import Sequential, Input, Model\n","from tensorflow.keras.layers import Dense, GRU, Embedding\n","from tensorflow.keras.initializers import Constant\n","GRU_model = Sequential()\n","GRU_model.add(Embedding(input_dim = vocab_size,\n","          output_dim = 300,\n","          embeddings_initializer = Constant(embedding_matrix),\n","          input_length = 256,\n","          mask_zero = True,\n","          trainable = False))\n","GRU_model.add(GRU(128, dropout = 0.5))\n","GRU_model.add(Dense(1, activation = 'sigmoid'))\n","GRU_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","source":["GRU_model.fit(train_padded, train_y.values, epochs = 20, batch_size = 128)"],"metadata":{"id":"QNPssvZtoBaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = GRU_model.predict(test_padded)"],"metadata":{"id":"84sCSAXhzVQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = np.array(tf.greater(pred, .5)).flatten()\n","(sum(pred == test_y)) / len(pred)"],"metadata":{"id":"DqzIML8Xz2Kh"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"project.ipynb","provenance":[],"history_visible":true,"authorship_tag":"ABX9TyNuvVXr/NxnUwUVYnYxtkqs"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}